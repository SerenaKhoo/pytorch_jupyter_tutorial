{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of content\n",
    "\n",
    "* [Autograd](#first-bullet)\n",
    "* [Tensor](#second-bullet)\n",
    "* [Function](#third-bullet)\n",
    "* [Examples](#forth-bullet)\n",
    "* [Gradients](#fifth-bullet)\n",
    "    * Calling the gradient with 1 element tensor \n",
    "    * Calling the gradient with more than 1 element tensor\n",
    "    * Stoping autograd from tracking the history on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to all network is the `autograd` package\n",
    "    - It provides automatic differentiation for all operations on Tensors. \n",
    "    - It is a define-by-run framework\n",
    "        - Your backprop is defined by how your code is run and that every single iteration can be different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor is the central class of the package \n",
    "    - .requires_grad if this is set as True, it will start to track all the operations on it.\n",
    "        - When we are done with our computation, we can call .backward() and have all the gradients computed automatically\n",
    "        - The gradient for this tensor will be accumulated into .grad attribute\n",
    "    \n",
    "    - To stop a tensor from tracking history, we have to call .detach()\n",
    "    - Or to prevent tracking history and using memory, we can wrapte the code block in {with torch.no_grad():}\n",
    "        - Useful when the code block has trainable parameters with requires_grad=True but we do not need it for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor and Function are interconnected and build up an ayclic graph that encodes a complete history of computation\n",
    "    - Each tensor has a .grad_fn attribute that references a Function that has created the Tensor\n",
    "        - If created by users, the .grad_fn = None\n",
    "    - If we want to compute the deriavatives, we call .backward() on a tensor \n",
    "    - If the Tensor is a scalar (ie, 1 element), we dont have to specify any arguments\n",
    "    - Else, we have to specify a gradient argument that is a tensor of matching shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples <a class=\"anchor\" id = \"forth-example\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tensor that requires gradient to track computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "\n",
      "grad_fn of x : \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(\"grad_fn of x : \")\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do an operation of tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, y was created because of an operation and not by the user --> Hence it has a grad_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000159912B3B00>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the requires_grad() for a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5000,  1.5000],\n",
      "        [ 1.5000,  1.5000]])\n",
      "False\n",
      "None\n",
      "\n",
      "a.requires_grad after calling the function is:\n",
      "True\n",
      "\n",
      "tensor(9.)\n",
      "<SumBackward0 object at 0x0000015994475780>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "a = ((a * 3) / (a  + 1)) \n",
    "print(a)\n",
    "print(a.requires_grad) # Because a was created by the user, it does not have a grad_fn\n",
    "print(a.grad_fn)\n",
    "print()\n",
    "\n",
    "# We can set requires_grad to be True by using the function requires_grad_ --> Defaults to true if True is not given\n",
    "a.requires_grad_(True)\n",
    "print(\"a.requires_grad after calling the function is:\")\n",
    "print(a.requires_grad)\n",
    "print()\n",
    "\n",
    "# b has grad_fn because it was created from an operation\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets FINALLY do a backprop now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12.,  12.],\n",
      "        [ 12.,  12.]])\n",
      "\n",
      "Creating out tensor\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad = True)\n",
    "y = x + 2\n",
    "z = y * y + 3 # Recall y = x + 2 and x = torch.ones\n",
    "print(z)\n",
    "print()\n",
    "\n",
    "print(\"Creating out tensor\")\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling backprop on a 1 element tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5000,  1.5000],\n",
      "        [ 1.5000,  1.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1513.8644,  -498.0468,  1011.6290])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: # math.sqrt(sum(element ** 2))\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  512.0000,  5120.0000,  2048.1128])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0, 0, 0], dtype = torch.float) # Just a random initialization\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stoping autograd from tracking history on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad= True)\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "print()\n",
    "\n",
    "# 1 way - detach X\n",
    "x = x.detach()\n",
    "print(x.requires_grad)\n",
    "\n",
    "# 2 way - wrap execution\n",
    "# We wrap the executions that do not require tracking under torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
