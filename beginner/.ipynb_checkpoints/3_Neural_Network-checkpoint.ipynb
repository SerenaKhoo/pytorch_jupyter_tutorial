{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network module\n",
    "import torch.nn.functional as F # Function \n",
    "\n",
    "import torch.optim as optim # Optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction and random notes](#notes)\n",
    "* [Define the network](#definition)\n",
    "* [Loss Function](#loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction and random notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral Networds can be constructed using the torch.nn package\n",
    "    - Depends on autograd to define models and differentiate them \n",
    "\n",
    "An nn.Module contains: \n",
    "    - layers \n",
    "    - A method: \n",
    "        - forward(input) that returns the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: MNIST - A network that classifies digit images (Below shows LeNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/beginner/mnist.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A typical training procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the neural network that has some learnable parameters (or weights)\n",
    "2. Iterate over a dataset of inputs\n",
    "3. Process input through the network \n",
    "4. Compute the loss (how far is the output from being correct) \n",
    "5. Propagate gradients back into the network's parameters \n",
    "6. Update the weights of the network, typically using a simple update rule: \n",
    "    - Weight = weight - learning_rate * gradients (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network <a class=\"anchor\" id=\"definition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.conv1 = nn.Conv2d(1,6,5)\n",
    "\n",
    "1 input channel, 6 output channels and 5 * 5 square convolution kernel \n",
    "\n",
    "* 1 --> Only 1 input channel. Ie, greyscale. There can be 3 input channels when we take in RGB. 4 if we do RGBA (Alpha: Transparency). In short, there can be multiple input channels. Not limited to the above\n",
    "\n",
    "* 6 -> 6 output channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module): # Our definition is the child of the parent class nn.module\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,6,5) # 1 input image channel, 6 output channels, 5*5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6,16,5) # 6 input channels\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self, x): # Does a max pooling over a (2,2) window \n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square, you can specify a single number \n",
    "        x = x.view(-1, self.num_flat_features(x)) # What is num_flat_features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # Excepted the size to have 3 elements --> first is the number of channels, then width, then height.\n",
    "        num_features = 1 # Hence, we only look at the number of elements by doing channels * width * height \n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to define the forward function, and the backward function (where gradients are computed) is \n",
    "automatically defined for you using autograd.\n",
    "\n",
    "We can use any of the Tensor operations in the foward function\n",
    "\n",
    "The learnable paramters of a model are returned by: net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnable parameters of a model = 10\n",
      "\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(\"Learnable parameters of a model = %d\"%len(params))\n",
    "print()\n",
    "\n",
    "for i in range(len(params)):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying it with a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.3439,  0.0954, -1.0675,  ..., -0.5767, -0.7132,  1.3258],\n",
      "          [-2.6607,  0.7291, -0.7603,  ...,  0.2788,  1.1863, -0.1948],\n",
      "          [-0.4967,  0.4298, -1.2249,  ...,  1.5445,  2.3858,  0.2340],\n",
      "          ...,\n",
      "          [ 0.3109,  0.2413,  0.6039,  ..., -1.4340, -1.6645,  1.0466],\n",
      "          [ 0.7392,  1.0077, -0.9612,  ...,  0.3058, -1.2804, -1.0641],\n",
      "          [-0.8854, -2.4237,  0.4854,  ...,  0.1305,  0.9162,  0.3988]]]])\n",
      "\n",
      "tensor([[-0.0413, -0.0071,  0.0168,  0.0099, -0.0689,  0.1562, -0.0518,\n",
      "          0.0458,  0.0920,  0.0400]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32) # 1 batch, 1 image with 32 * 32 --> Because torch.nn only supports inputs that are a mini-batch \n",
    "print(input) # of samples and not a single sample. --> Hence it takes nSamples x nChannels x Height x Width \n",
    "print()\n",
    "\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() # Clear the buffer at each batch \n",
    "out.backward(torch.rand(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function <a class=\"anchor\" id=\"loss\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0413, -0.0071,  0.0168,  0.0099, -0.0689,  0.1562, -0.0518,\n",
      "          0.0458,  0.0920,  0.0400]])\n",
      "Before: \n",
      "tensor([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])\n",
      "\n",
      "After: \n",
      "tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]])\n",
      "\n",
      "tensor(38.1327)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "print(output)\n",
    "\n",
    "target = torch.arange(1,11) # ie, range from 0 to 10 \n",
    "print(\"Before: \")\n",
    "print(target)\n",
    "print()\n",
    "target = target.view(1, -1)\n",
    "print(\"After: \")\n",
    "print(target)\n",
    "print()\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this: \n",
    "    \n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "\n",
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000018E4A2B4F60>\n",
      "<AddmmBackward object at 0x0000018E4A2B4518>\n",
      "<ExpandBackward object at 0x0000018E4A2B4F60>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear function \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call loss.backward(), and have a look at conv1â€™s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0353, -0.0117,  0.0400,  0.0148, -0.2055,  0.0660])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "    \n",
    "    weight = weight - learning_rate * gradient\n",
    "    \n",
    "We can use the torch.optim that implements all these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# To update the weights, we can do the following: \n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target) # We have placed the criterion to be MSE Loss \n",
    "loss.backward() # To do the backprop\n",
    "optimizer.step() # Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
